{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BoLTBdYRJJtw"
      },
      "outputs": [],
      "source": [
        "!pip install vit-pytorch>=0.40.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install coca-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1m1A84JRt6",
        "outputId": "5773b3b7-90ab-40fa-ae15-f566689adc63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting coca-pytorch\n",
            "  Downloading CoCa_pytorch-0.1.0-py3-none-any.whl.metadata (747 bytes)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.12/dist-packages (from coca-pytorch) (0.8.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.12/dist-packages (from coca-pytorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->coca-pytorch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6->coca-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6->coca-pytorch) (3.0.3)\n",
            "Downloading CoCa_pytorch-0.1.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: coca-pytorch\n",
            "Successfully installed coca-pytorch-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# import vision transformer\n",
        "\n",
        "from vit_pytorch.simple_vit_with_patch_dropout import SimpleViT\n",
        "from vit_pytorch.extractor import Extractor"
      ],
      "metadata": {
        "id": "lpre21X5JjQJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = SimpleViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    patch_dropout = 0.5  # https://arxiv.org/abs/2212.00794\n",
        ")\n",
        "\n",
        "vit = Extractor(vit, return_embeddings_only = True, detach = False)\n",
        "\n",
        "# extractor will enable it so the vision transformer returns its embeddings\n"
      ],
      "metadata": {
        "id": "PQADpA71Jmf3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import CoCa and instantiate it\n",
        "\n",
        "from coca_pytorch.coca_pytorch import CoCa\n",
        "\n",
        "coca = CoCa(\n",
        "    dim = 512,                     # model dimension\n",
        "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
        "    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n",
        "    num_tokens = 20000,            # number of text tokens\n",
        "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
        "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
        "    dim_head = 64,                 # dimension per attention head\n",
        "    heads = 8,                     # number of attention heads\n",
        "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
        "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
        ").cuda()"
      ],
      "metadata": {
        "id": "Tf5gwemkJqQY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mock text and images\n",
        "\n",
        "text = torch.randint(0, 20000, (4, 512)).cuda()\n",
        "images = torch.randn(4, 3, 256, 256).cuda()\n",
        "\n",
        "text.shape, images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZciihHoJvfx",
        "outputId": "c1e0e922-9ed7-4018-8f91-8c82cc34fc77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 512]), torch.Size([4, 3, 256, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train by giving CoCa your text and images with `return_loss = True`\n",
        "\n",
        "loss = coca(\n",
        "    text = text,\n",
        "    images = images,\n",
        "    return_loss = True  # set this to True to get the full caption + contrastive loss\n",
        ")\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# do the above for as much text and images...\n",
        "# then you can get the caption logits as so\n",
        "\n",
        "logits = coca(\n",
        "    text = text,\n",
        "    images = images\n",
        ") # (4, 512, 20000)\n",
        "\n",
        "# and the CLIP-like text and image embeddings as\n",
        "\n",
        "text_embeds, image_embeds = coca(\n",
        "    text = text,\n",
        "    images = images,\n",
        "    return_embeddings = True\n",
        ") # (4, 512), (4, 512)"
      ],
      "metadata": {
        "id": "1EzeeKYCJWKE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeds.shape, image_embeds.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU46U3dlKXya",
        "outputId": "54221ffa-73c5-4bda-dac0-43d4b7020024"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 512]), torch.Size([4, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}